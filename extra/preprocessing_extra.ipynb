{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Extra tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience functions for reading in data\n",
    "\n",
    "Here, we define a bunch of functions that simplify the process of reading in data that we'll use throughout today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "def read_pride():\n",
    "    fname = os.path.join(DATA_DIR, 'pride-and-prejudice.txt')\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_trump():\n",
    "    fname = os.path.join(DATA_DIR, 'trump-tweets.csv')\n",
    "    df = pd.read_csv(fname)\n",
    "    return list(df['Tweet_Text'].values)\n",
    "\n",
    "def read_austen():\n",
    "    fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "    fnames = glob.glob(fnames)\n",
    "    austen = ''\n",
    "    for fname in fnames:\n",
    "        with open(fname) as f:\n",
    "            text = f.read()\n",
    "            austen += text\n",
    "    return austen\n",
    "\n",
    "def read_amazon(n=2):   \n",
    "    fnames = os.path.join(DATA_DIR, 'amazon', '*.csv')\n",
    "    fnames = glob.glob(fnames)\n",
    "    reviews = []\n",
    "    column_names = ['id', 'product_id', 'user_id', 'profile_name', 'helpfulness_num', 'helpfulness_denom',\n",
    "                   'score', 'time', 'summary', 'text']\n",
    "    for fname in fnames[:n]:\n",
    "        df = pd.read_csv(fname, names=column_names)\n",
    "        text = list(df['text'].iloc[1:])\n",
    "        reviews.extend(text)\n",
    "    return reviews\n",
    "\n",
    "def read_dante():\n",
    "    fname = os.path.join(DATA_DIR, 'dante.txt')\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_example(n=1):\n",
    "    fname = os.path.join(DATA_DIR, 'example{}.txt'.format(n))\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "    \n",
    "def read_music():\n",
    "    fname = os.path.join(DATA_DIR, 'music_reviews.csv')\n",
    "    return list(pd.read_csv(fname, sep='\\t')['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files\n",
    "\n",
    "The first step is to read in the files containing the data. As we discussed last week, the most common file types for text data are: `.txt`, `.csv`, `.json`, `.html` and `.xml`.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "- What type of object is `raw`?\n",
    "- How many characters are in `raw`?\n",
    "- Get the first 1000 characters of `raw`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = '../day-1/data'\n",
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.csv`\n",
    "\n",
    "Python has a built-in module called `csv` for reading in csv files.\n",
    "\n",
    "- What type is `tweets`?\n",
    "- How many entries are in `raw`?\n",
    "- Which entry is the header row?\n",
    "- How can we get the text of the first question?\n",
    "- How can we get a list of the texts of all questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = []\n",
    "with open(fname) as f:\n",
    "    reader = csv.reader(f)\n",
    "    tweets = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.csv` with `pandas`\n",
    "\n",
    "`pandas` is a third-party library that makes working with tabular data much easier. This is the recommended way to read in a `.csv` file.\n",
    "\n",
    "- How many tweets are there?\n",
    "- What happened to the header row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = list(tweets['Tweet_Text'])\n",
    "tweet_text[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.json` files\n",
    "\n",
    "Python has built-in support for reading in `.json` files.\n",
    "\n",
    "- How many questions are there in the dataset?\n",
    "- What data type is each question?\n",
    "- How can we access the question text of the first question?\n",
    "- How can we get a list of the texts of all questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# fname = 'jeopardy.json'\n",
    "# fname = os.path.join(DATA_DIR, fname)\n",
    "# with open(fname) as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.html` files\n",
    "\n",
    "The best way to read in `.html` files in Python is with the `BeautifulSoup` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "fname = 'time.html'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    html = f.read()\n",
    "    soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = soup.findAll(text=True)\n",
    "#texts = soup.getText()\n",
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in `.xml` files\n",
    "\n",
    "We read in `.xml` files using the `ElementTree` module of Python's standard library. We can think of `.xml` files as trees where each branch has a tag name. We can find all the branches with a certain name as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "fname = 'books.xml'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "e = ET.parse(fname)\n",
    "root = e.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = root.findall('*/description')\n",
    "text = [d.text for d in descriptions]\n",
    "text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We want to be able to read them all into a single variable.\n",
    "\n",
    "- What type is `austen`?\n",
    "- What type is `fnames` after it is first assigned a value?\n",
    "- What type is `fnames` after it is assigned a second value?\n",
    "- How "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "austen = ''\n",
    "for fname in fnames:\n",
    "    with open(fname) as f:\n",
    "        text = f.read()\n",
    "        austen += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character encoding\n",
    "\n",
    "Character encoding was more of a problem in Python 2 and early years in general. With Python 3 and most text files being encoded in `UTF-8`, we don't often need to think about it. If you're getting nonsense when reading in a file, try adding `encoding='utf-8'` to the `open` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dante.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'akutagawa.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[5000:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision\n",
    "\n",
    "I've read in the text of Jane Austen's _Pride and Prejudice_ into a variable called `pride`. Your tasks are to:\n",
    "- Figure out what type of Python object `pride` is.\n",
    "- Tokenize the text and store it in a variable called `tokenized_pride`.\n",
    "- Figure out what type `tokenized_pride` is.\n",
    "- Remove all punctuation from `pride`.\n",
    "- Remove all punctuation from `tokenized_pride`.\n",
    "- Break `pride` up into sentences and store the result as `sents_pride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride = read_pride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV words\n",
    "\n",
    "Sometimes it's best for us to remove infrequent words (sometimes not!). When we do remove infrequent words, it's often for a downstream method (like classification) that is sensitive to rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "all_tweets = ' '.join(tweets)\n",
    "\n",
    "# define regex patterns for cleaning\n",
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "URL_SIGN = ' URL '\n",
    "hashtag_pattern = r'(?:^|\\s)[ï¼ƒ#]{1}(\\w+)'\n",
    "HASHTAG_SIGN = ' HASHTAG '\n",
    "digit_pattern = '\\d+'\n",
    "DIGIT_SIGN = ' DIGIT '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = re.sub(url_pattern, URL_SIGN, all_tweets)\n",
    "clean = re.sub(hashtag_pattern, HASHTAG_SIGN, clean)\n",
    "clean = re.sub(digit_pattern, DIGIT_SIGN, clean)\n",
    "tokens = word_tokenize(clean)\n",
    "tokens = [token for token in tokens if token not in punctuation]\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the frequency of each word type with the built-in `Counter` in Python. This basically just takes the set of word types (we calculated this above as `vocabulary`) and makes a special Python dictionary with each value being the number of times it appears in the list. We can ask that dictionary for the most common words, or for the frequency of individual word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq['unleashed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV = 'OOV'\n",
    "new_tokens = []\n",
    "for token in tokens:\n",
    "    if freq[token] == 1:\n",
    "        new_tokens.append(OOV)\n",
    "    else:\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis? Today, we'll learn one simple approach to this: TF-IDF. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is `tf-idf score`. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "Traditionally, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. This function also uses log frequencies, so the numbers will not correspond excactly to the calculations above. We'll use the [scikit-learn calculation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but a challenge for you: use Pandas to calculate this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_pattern = r'\\s+'\n",
    "clean = [re.sub(url_pattern, URL_SIGN, t) for t in tweets]\n",
    "clean = [re.sub(hashtag_pattern, HASHTAG_SIGN, t) for t in clean]\n",
    "clean = [re.sub(digit_pattern, DIGIT_SIGN, t) for t in clean]\n",
    "clean = [re.sub(whitespace_pattern, ' ', t) for t in clean]\n",
    "clean[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = read_music()\n",
    "music[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digit(comment):\n",
    "    return ''.join([ch for ch in comment if not ch.isdigit()])\n",
    "\n",
    "no_digits = [remove_digit(comment) for comment in music]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDFVectorizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "sparse_tfidf = tfidfvec.fit_transform(no_digits)\n",
    "sparse_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(sparse_tfidf.toarray(), columns=tfidfvec.get_feature_names())\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add in a column of genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(DATA_DIR, 'music_reviews.csv')\n",
    "reviews = pd.read_csv(fname, sep='\\t')\n",
    "\n",
    "tfidf['genre_'] = reviews['genre']\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap = tfidf[tfidf['genre_']=='Rap']\n",
    "indie = tfidf[tfidf['genre_']=='Indie']\n",
    "jazz = tfidf[tfidf['genre_']=='Jazz']\n",
    "\n",
    "rap.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indie.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz.max(numeric_only=True).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference of proportions\n",
    "\n",
    "A simple way to calculate distinctive words in two texts (see also TF-IDF in day 2 notebook) is to calculate the words with the highest and lowest difference or proportions. In theory frequent words like 'the' and 'of' will have a small difference. In practice this doesn't happen.\n",
    "\n",
    "To demonstrate this we will run a difference of proportion calculation on *Pride and Prejudice* and *A Garland for Girls*.\n",
    "\n",
    "To get the text in shape for scikit-learn we need to creat a list object with each novel as an element in a list. We'll use the append function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text_list = []\n",
    "#open and read the novels, save them as variables\n",
    "austen_string = open('../day-2/data/Austen_PrideAndPrejudice.txt', encoding='utf-8').read()\n",
    "alcott_string = open('../day-2/data/Alcott_GarlandForGirls.txt', encoding='utf-8').read()\n",
    "\n",
    "#append each novel to the list\n",
    "text_list.append(austen_string)\n",
    "text_list.append(alcott_string)\n",
    "print(text_list[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat a DTM from these two novels, force it into a pandas DF, and inspect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "novels_df = pd.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the number of rows and columns.\n",
    "\n",
    "Question: What does this mean?\n",
    "\n",
    "Next, we need to get a word frequency count for each novel, which we can do by summing across the entire row. Note how the syntax is different here compared to when we summed one column across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df['word_count'] = novels_df.sum(axis=1)\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we divide each frequency cell by the word count. This syntax gets a bit tricky, so let's walk through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df = novels_df.iloc[:,:].div(novels_df.word_count, axis=0)\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we subtract one row from another, and add the output as a third row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df.loc[2] = novels_df.loc[0] - novels_df.loc[1]\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort based of the values of this row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df.loc[2].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are still in there. Why?\n",
    "\n",
    "We can, of course, manually remove stop words. This does successfully identify distinctive content words. \n",
    "\n",
    "We can do this in the CountVectorizer step, by setting the correct option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change stop_words option to 'english\n",
    "countvec_sw = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#same as code above\n",
    "novels_df_sw = pd.DataFrame(countvec_sw.fit_transform(text_list).toarray(), columns=countvec_sw.get_feature_names())\n",
    "novels_df_sw['word_count'] = novels_df_sw.sum(axis=1)\n",
    "novels_df_sw = novels_df_sw.iloc[:,0:].div(novels_df_sw.word_count, axis=0)\n",
    "novels_df_sw.loc[2] = novels_df_sw.loc[0] - novels_df_sw.loc[1]\n",
    "novels_df_sw.loc[2].sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this by setting the max_df option (maximum document frequency) to either an absolute value, or a decimal between 0 and 1. An absolute value indicate that if the word occurs in more documents than the stated value, that word **will not** be included in the DTM. A decimal value will do the same, but proportion of documents.\n",
    "\n",
    "Question: In the case of this corpus, what does setting the max_df value to 1 do? What output do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change max_df option to 1\n",
    "countvec_freq = CountVectorizer(max_df=1)\n",
    "\n",
    "#same as the code above\n",
    "novels_df_freq = pd.DataFrame(countvec_freq.fit_transform(text_list).toarray(), columns=countvec_freq.get_feature_names())\n",
    "novels_df_freq['word_count'] = novels_df_freq.sum(axis=1)\n",
    "novels_df_freq = novels_df_freq.iloc[:,0:].div(novels_df_freq.word_count, axis=0)\n",
    "novels_df_freq.loc[2] = novels_df_freq.loc[0] - novels_df_freq.loc[1]\n",
    "novels_df_freq.loc[2].sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What would happen if we set the max_df to 2, in this case?\n",
    "\n",
    "Question: What might we do for the music reviews dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use the difference of proportions calculation to compare two genres, or two artists, in the music reviews dataset. There are many ways you can do this. Think through the problem in steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
